<!doctype html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Every hero has a code.">
<meta property="og:type" content="website">
<meta property="og:title" content="Shaw's Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Shaw's Blog">
<meta property="og:description" content="Every hero has a code.">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Shaw's Blog">
<meta name="twitter:description" content="Every hero has a code.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title> Shaw's Blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  














  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Shaw's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/28/记一次面试经历/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe Shaw">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shaw's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/09/28/记一次面试经历/" itemprop="url">
                  记一次面试经历
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-28T19:14:36+08:00">
                2017-09-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="day-1-亚马逊实习面试。"><a href="#day-1-亚马逊实习面试。" class="headerlink" title="day 1 亚马逊实习面试。"></a>day 1 亚马逊实习面试。</h1><p>研一上学期实验室没事做，研究生课程又很水，所以想出去找个实习。给亚马逊投了，约了面试。<br>亚马逊离学校有点远，在远洋国际中心，公交坐了一个小时。</p>
<p>一面：是个小哥哥，上来就做题。。。一个数组里找和为k的两个数的下标。虽然现在回忆起来是很简单的一道题就是two sum啊(leetcode easy级别题目），但是当时我也没给出最优解法。只想出了O(nlogn)的，其实o(n)的解法，就是设置两个指针，然后一个从前一个从后，比较下就可以了。。。哎，然后就问了些项目相关</p>
<p>二面：感觉是个leader，就主要问了项目，然后问了怎么利用机器学习去预测热点等。。</p>
<p>总体下来感觉应该挂了。</p>
<p>总结：应该好好练算法，把leetcode刷穿！好嘛</p>
<h1 id="day2-IBM实习面试"><a href="#day2-IBM实习面试" class="headerlink" title="day2 IBM实习面试"></a>day2 IBM实习面试</h1><p>再次感叹下中关村软件园好远。。。地铁＋公交，几经周折终于到了面试地点。<br>软件园环宇大厦，一看建筑就觉得浓浓的国企的氛围。<br>然后直接是面试官接待的，然后就一直聊。。问简历，问了TF-IDF为什么表现好（这个我也忘记了。。。）说觉得我算法做的比较多，开发做的少。问了最小生生成树的算法（real 忘记了）。然后还问了C++的虚函数（real 忘记了）。说他们部门现在在做一个聊天机器人，需要构件知识图谱。<br>面试官全城笑脸，态度很好，最后面试完还送我到了门口。</p>
<p>总结：基础不扎实，对自己的项目不是很熟悉，还有觉得自己应该多做些开发相关的，比如web，大数据平台等。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/27/线段树与字典树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe Shaw">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shaw's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/09/27/线段树与字典树/" itemprop="url">
                  线段树与字典树
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-27T09:44:16+08:00">
                2017-09-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="线段树"><a href="#线段树" class="headerlink" title="线段树"></a>线段树</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>线段树，类似区间树，它在各个节点保存一条线段（数组中的一段子数组），主要用于高效解决连续区间的动态查询问题，由于二叉结构的特性，它基本能保持每个操作的复杂度为O(logn)。<br>线段树的每个节点表示一个区间，子节点则分别表示父节点的左右半区间，例如父亲的区间是[a,b]，那么(c=(a+b)/2)左儿子的区间是[a,c]，右儿子的区间是[c+1,b]。</p>
<ul>
<li>主要操作：构建线段树、区间查询、区间修改</li>
</ul>
<h2 id="运用"><a href="#运用" class="headerlink" title="运用"></a>运用</h2><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>例题：leetcode<a href="https://leetcode.com/problems/range-sum-query-mutable/description/" target="_blank" rel="external">307. Range Sum Query - Mutable</a></p>
<p>代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div></pre></td><td class="code"><pre><div class="line">public class NumArray &#123;</div><div class="line">       class SegmentTreeNode &#123;</div><div class="line">           int start, end;</div><div class="line">           SegmentTreeNode left;</div><div class="line">           SegmentTreeNode right;</div><div class="line">           int sum;</div><div class="line">           public SegmentTreeNode(int start, int end) &#123;</div><div class="line">               this.start = start;</div><div class="line">               this.end = end;</div><div class="line">               this.left = null;</div><div class="line">               this.right = null;</div><div class="line">               this.sum = 0;</div><div class="line">           &#125;</div><div class="line">       &#125;</div><div class="line">       SegmentTreeNode root = null;</div><div class="line"></div><div class="line">       public NumArray(int[] nums) &#123;</div><div class="line">           root = buildTree(nums, 0, nums.length - 1);</div><div class="line">       &#125;</div><div class="line">       public SegmentTreeNode buildTree(int[] nums, int start, int end) &#123;</div><div class="line">           if (start &gt; end) &#123;</div><div class="line">               return null;</div><div class="line">           &#125; else &#123;</div><div class="line">               SegmentTreeNode node = new SegmentTreeNode(start, end);</div><div class="line">               if (start == end) &#123;</div><div class="line">                   node.sum = nums[start];</div><div class="line">               &#125; else &#123;</div><div class="line">                   int mid = start + (end - start) / 2;</div><div class="line">                   node.left = buildTree(nums, start, mid);</div><div class="line">                   node.right = buildTree(nums, mid + 1, end);</div><div class="line">                   node.sum = node.left.sum + node.right.sum;</div><div class="line">               &#125;</div><div class="line">               return node;</div><div class="line">           &#125;</div><div class="line">       &#125;</div><div class="line">       public void update(int i, int val) &#123;</div><div class="line">           update(root, i, val);</div><div class="line">       &#125;</div><div class="line">       public void update(SegmentTreeNode root, int pos, int val) &#123;</div><div class="line">           if (root.start == root.end) &#123;</div><div class="line">               root.sum = val;</div><div class="line">           &#125; else &#123;</div><div class="line">               int mid = root.start + (root.end - root.start) / 2;</div><div class="line">               if (mid &gt;= pos) &#123;</div><div class="line">                   update(root.left, pos, val);</div><div class="line">               &#125; else &#123;</div><div class="line">                   update(root.right, pos, val);</div><div class="line">               &#125;</div><div class="line">               root.sum = root.left.sum + root.right.sum;</div><div class="line">           &#125;</div><div class="line">       &#125;</div><div class="line">       public int sumRange(int i, int j) &#123;</div><div class="line">           return sumRange(root, i, j);</div><div class="line">       &#125;</div><div class="line">       public int sumRange(SegmentTreeNode root, int start, int end) &#123;</div><div class="line">           if (root.start == start &amp;&amp; root.end == end) &#123;</div><div class="line">               return root.sum;</div><div class="line">           &#125; else &#123;</div><div class="line">               int mid = root.start + (root.end - root.start) / 2;</div><div class="line">               if (end &lt;= mid) &#123;</div><div class="line">                   return sumRange(root.left, start, end);</div><div class="line">               &#125; else if (start &gt;= mid + 1) &#123;</div><div class="line">                   return sumRange(root.right, start, end);</div><div class="line">               &#125; else &#123;</div><div class="line">                   return sumRange(root.left, start, mid) + sumRange(root.right, mid + 1, end);</div><div class="line">               &#125;</div><div class="line">           &#125;</div><div class="line">       &#125;</div><div class="line">   &#125;</div></pre></td></tr></table></figure>
<h1 id="字典树"><a href="#字典树" class="headerlink" title="字典树"></a>字典树</h1><h2 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h2><p>字典书是一种单词查找树，利用单词的公共前缀来构造的用于快速检索的多叉树。<br>trie树的基本性质：</p>
<ul>
<li>根节点不包含字符，除根节点外的每一个子节点都包含一个字符。</li>
<li>从根节点到某一个节点，路径上经过的字符连接起来，为该节点对应的字符串。</li>
<li>每个节点的所有子节点包含的字符互不相同。<br>实现方法：字母的字典树每个节点要定义一个大小为26的子节点数组，然后用一个标志符isEnd用来记录到当前位置是否为结束位置，即为止是否为一个词。</li>
</ul>
<h2 id="运用-1"><a href="#运用-1" class="headerlink" title="运用"></a>运用</h2><p>典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是最大限度地减少无谓的字符串比较，查询效率比较高。<br>典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。优点是最大限度地减少无谓的字符串比较，查询效率比较高。<br>Trie的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。</p>
<h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><p>实例：leetcode <a href="https://leetcode.com/problems/implement-trie-prefix-tree/description/" target="_blank" rel="external">208. Implement Trie (Prefix Tree)</a></p>
<ul>
<li>insert方法实现： 可以用index记录一个当前的word插入到第几个单词了。如果index == word.length，插入结束，且将目前单词的isEnd设为true；否则就将word当前字母取出来，并且看子节点是否在children中存在，即可以利用word.charAt(index) - ‘a’的形式形成字母到字母在该根节点的子节点数组中位置的映射。如果为空，则创建新节点。不为空则继续插入下一个节点。</li>
<li>search和startWith方法都可以用find方法来实现，不过一个需要判断下是否找到的当前节点为完成单词（isend ＝＝ true？）；find的时候如果哦当前点在子节点数组中为空，则返回null</li>
</ul>
<p>代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">class TrieNode &#123;</div><div class="line">        private TrieNode[] children;</div><div class="line">        public boolean isEnd;</div><div class="line">        public TrieNode() &#123;</div><div class="line">            children = new TrieNode[26];</div><div class="line">            isEnd = false;</div><div class="line">        &#125;</div><div class="line">        public void insert(String word, int index) &#123;</div><div class="line">            if (index == word.length()) &#123;</div><div class="line">                this.isEnd = true;</div><div class="line">                return;</div><div class="line">            &#125;</div><div class="line">            int pos = word.charAt(index) - &apos;a&apos;;</div><div class="line">            if (children[pos] == null)</div><div class="line">                children[pos] = new TrieNode();</div><div class="line">            children[pos].insert(word, index + 1);</div><div class="line">        &#125;</div><div class="line">        public TrieNode find(String word, int index) &#123;</div><div class="line">            if (index == word.length())</div><div class="line">                return this;</div><div class="line">            int pos = word.charAt(index) - &apos;a&apos;;</div><div class="line">            if (children[pos] == null)</div><div class="line">                return null;</div><div class="line">            return children[pos].find(word, index + 1);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    class Trie &#123;</div><div class="line">        TrieNode root;</div><div class="line"></div><div class="line">        /** Initialize your data structure here. */</div><div class="line">        public Trie() &#123;</div><div class="line">            root = new TrieNode();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        /** Inserts a word into the trie. */</div><div class="line">        public void insert(String word) &#123;</div><div class="line">            if (word.length() == 0)</div><div class="line">                return;</div><div class="line">            root.insert(word, 0);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        /** Returns if the word is in the trie. */</div><div class="line">        public boolean search(String word) &#123;</div><div class="line">            if (word.length() == 0)</div><div class="line">                return false;</div><div class="line">            TrieNode node = root.find(word, 0);</div><div class="line">            return (node != null &amp;&amp; node.isEnd);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        /** Returns if there is any word in the trie that starts with the given prefix. */</div><div class="line">        public boolean startsWith(String prefix) &#123;</div><div class="line">            if (prefix.length() == 0)</div><div class="line">                return false;</div><div class="line">            TrieNode node = root.find(prefix, 0);</div><div class="line">            return node != null;</div><div class="line">        &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/23/Binary-Search类型问题解题模版/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe Shaw">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shaw's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/09/23/Binary-Search类型问题解题模版/" itemprop="url">
                  Binary Search类型问题解题模版
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-23T20:52:24+08:00">
                2017-09-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>对于需要找某元素首次/末次出现的位置？first position, last position</p>
<h2 id="1-需要注意的点："><a href="#1-需要注意的点：" class="headerlink" title="1.需要注意的点："></a>1.需要注意的点：</h2><ul>
<li>循环结束的条件：start + 1 &lt; end : 当循环进行到数组里只有两个数时，即当start与end相邻时。里面就不用加1和减1</li>
<li>使用 mid = start + (end - start) / 2 而不是 mid = (start + end) / 2，因为后者可能会导致溢出（细节）</li>
<li>然后比较A[mid] == , &lt;, &gt; : 如果是要找last position, 当A[mid] == target时，执行start = mid，而不是直接返回;如果找first position ，则要执行end = mid,且后面判断的时候先判断A[start]</li>
<li>结束循环最后还需比较A[start]和A[end]哪个是target，如果需要的是找最后一个出现的target，则需要先判断A[end];如果需要找第一个出现的target，则首先需要判断A[start]是不是target</li>
</ul>
<p>====&gt;&gt; 任何二分法的问题都可以归为找first position和last position</p>
<h2 id="2-看到O-logn-复杂度，一般考虑用二分法"><a href="#2-看到O-logn-复杂度，一般考虑用二分法" class="headerlink" title="2.看到O(logn)复杂度，一般考虑用二分法"></a>2.看到O(logn)复杂度，一般考虑用二分法</h2><ul>
<li>如果问题用O(n)可以解决，但是面试官不满意，则要找O(logn)的方法</li>
</ul>
<h2 id="3-旋转数组的问题：-旋转数组的特点：有两个上升区间-画图），特殊情况一个上升区间）"><a href="#3-旋转数组的问题：-旋转数组的特点：有两个上升区间-画图），特殊情况一个上升区间）" class="headerlink" title="3.旋转数组的问题：(旋转数组的特点：有两个上升区间(画图），特殊情况一个上升区间）"></a>3.旋转数组的问题：(旋转数组的特点：有两个上升区间(画图），特殊情况一个上升区间）</h2><ul>
<li>没有重复元素时：可以用二分法 : 怎样划分区间 （二分法实质上是对区间的划分，每次去掉一部分空间）</li>
<li>有重复元素：不能用二分法，因为有重复元素时实现的复杂度最坏就是O(n)，证明：黑盒测试＝&gt; 假设前n-1次取出来的都是相同的，则不能得到任何有用的信息，第n次取到了最小的，才能知道最小的元素的位置。所以存在重复元素时的时间复杂度最小都是o(n)，即不存在o(logn)的解法。</li>
</ul>
<p>eg: 如题目：search in sorted array</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">class Solution &#123;</div><div class="line">    public int search(int[] nums, int target) &#123;</div><div class="line">        if (nums == null || nums.length == 0) &#123;</div><div class="line">            return -1;</div><div class="line">        &#125;</div><div class="line">        int start = 0, end = nums.length - 1;</div><div class="line">        int mid;</div><div class="line">        while (start + 1 &gt; end) &#123;</div><div class="line">            mid = start + (end - start) / 2;</div><div class="line">            if (nums[mid] == target) &#123;</div><div class="line">                return mid;</div><div class="line">            &#125; else if (nums[mid] &gt; nums[start]) &#123;</div><div class="line">                if (target &gt;= nums[start] &amp;&amp; target &lt;= nums[mid]) &#123;</div><div class="line">                    end = mid;</div><div class="line">                &#125; else &#123;</div><div class="line">                    start = mid;</div><div class="line">                &#125;</div><div class="line">            &#125; else &#123;</div><div class="line">                if (target &gt;= nums[mid] &amp;&amp; target &lt;= nums[end]) &#123;</div><div class="line">                    start = mid;</div><div class="line">                &#125; else &#123;</div><div class="line">                    end = mid;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        if (nums[start] == target) &#123;</div><div class="line">            return start;</div><div class="line">        &#125; else if (nums[end] == target) &#123;</div><div class="line">            return end;</div><div class="line">        &#125;</div><div class="line">        return -1;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/ZoeShaw101/ZoeShaw101.github.io/master/images/Snip20170916_10.png" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/21/计算语言学基础－读书笔记（一）词法分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe Shaw">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shaw's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/09/21/计算语言学基础－读书笔记（一）词法分析/" itemprop="url">
                  计算语言学基础－读书笔记（一）词法分析
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-21T10:20:41+08:00">
                2017-09-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="正则表达式与自动机"><a href="#正则表达式与自动机" class="headerlink" title="正则表达式与自动机"></a>正则表达式与自动机</h1><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><h3 id="1-常用符号"><a href="#1-常用符号" class="headerlink" title="1.常用符号"></a>1.常用符号</h3><ul>
<li>/[]/ : 用来匹配任意的字符</li>
<li>/[-]/ : 中的-用来表示字符的范围</li>
<li>/[^]/ : 中的^用来表示匹配除去某模式的串（仅当^在左括号[的第一个出现时才能当作除去匹配的作用）</li>
<li>/?/ question-mark: 匹配零个或一个前面的字符，意思是?前面的那个字符是可选的（即可要可不要）</li>
<li>/<em>/ 星号模式：用来匹配零个或多个前面的字符。当字符直邮一个时。比如a</em>，还可以匹配完全不包含a的仁义字符串的起始处，所以当要匹配一个或多个a是，应该用aa<em>；ab</em>表示可以匹配一个或多个a或b</li>
<li>/+/ 加号模式：匹配一个或多个前面的字符；与？和*的区别是+不会匹配零个</li>
<li><p>/./ 句点模式：匹配任意的字符。和<em>组合表示匹配任意的字符串 /ab.</em>ab/</p>
<p><strong><em>anchors</em></strong>：<br>anchors是特殊的字符串来匹配字符串中的特定模式</p>
</li>
<li><p>/^$/ : ^表示在行的开始处进行匹配，$表示在行的末尾处进行匹配</p>
</li>
<li>/\b/ : 用来匹配单词边界，它通常用来避免意外地匹配到在其他单词内的某个单词. ；/\B/ : 用来匹配非单词边界</li>
</ul>
<h3 id="2-disjunction-grouping-and-precedence"><a href="#2-disjunction-grouping-and-precedence" class="headerlink" title="2.disjunction,grouping and precedence"></a>2.disjunction,grouping and precedence</h3><ul>
<li>/cat|dog/: 使用｜来表示匹配左边的模式或者右边的模式</li>
<li>/()/ : 使用()来确定求值的顺序</li>
</ul>
<h3 id="3-表达式优先级："><a href="#3-表达式优先级：" class="headerlink" title="3.表达式优先级："></a>3.表达式优先级：</h3><p>() &gt; +?{} &gt; ^$ &gt; |</p>
<h3 id="4-高级操作"><a href="#4-高级操作" class="headerlink" title="4.高级操作"></a>4.高级操作</h3><ul>
<li>\d : [0–9]</li>
<li>\D : [^0-9]</li>
<li>\w : [a-zA-Z0-9_]</li>
<li>\W : [^\w]</li>
<li>\s : [\r\t\n\f]</li>
<li>\S : [^\s]</li>
</ul>
<h2 id="自动机"><a href="#自动机" class="headerlink" title="自动机"></a>自动机</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h3><p><strong><em>确定性有限自动机 DFSA</em></strong>:</p>
<ul>
<li>有限状态集合S；</li>
<li>有限输入符号的字母表Σ；</li>
<li>状态转移函数move；</li>
<li>开始状态 s0；</li>
<li>结束状态集合F，F ∈ S。 自动机初始状态为s0，逐一读入输入字符串中的每一个字母，根据当前状态、读入的字母， 由状态转移函数move控制进入下一个状态。如果输入字符串读入结束时自动机的状态属于结束状态集合F， 则说明该自动机接受该字符串，否则为不接受。</li>
</ul>
<p><strong><em>不确定性有限自动机 NFSA</em></strong>: 存在空的输入或者每个状态的转移存在多种路径</p>
<h3 id="2-算法"><a href="#2-算法" class="headerlink" title="2.算法"></a>2.算法</h3><p>不确定性FSA的解决方法：</p>
<ul>
<li>Backup</li>
<li>Look-ahead</li>
<li>Parallelism</li>
</ul>
<p>Backup方法：NFSA算法：</p>
<p>状态空间搜索算法：就是将所有可能进行转移的状态存下来，选择一条路径结果reject时就回溯回分叉路口来选取另一条路径。使得搜索的状态有序，可以使用深搜或宽搜算法。但是这两种算法都有弊端：当状态空间无尽时搜索就不会停止，而且当状态空间很你很大时，会耗费大量的存储空间。更复杂高效的算法可以使用动态规划或者A*算法。<br><img src="https://raw.githubusercontent.com/ZoeShaw101/ZoeShaw101.github.io/master/images/Snip20170921_1.png" alt=""></p>
<h2 id="使用正则表达式建立自动机"><a href="#使用正则表达式建立自动机" class="headerlink" title="使用正则表达式建立自动机"></a>使用正则表达式建立自动机</h2><p>RE描述了一个定义在某个字母表Σ上的字符串集合L,并且空字符串ε也属于L集合.RE有三个基本的操作:<br>(1)选择 取并集.符号:|. 比如两个字符串集合R和S的选择操作,记作R|S.<br>(2)连接 字符串之间的拼接.两个字符串集合R和S的连接为RS.<br>(3)闭包 符号:<em> 字符串集合R的闭包R</em>是指把R与自身连接零次或者多次形成的所有集合的并集.<br>  由这几个简单的操作可以得到我们平常接触的正则表达式的所有扩展.</p>
<p>如下图所示的几种转化：<br><img src="http://upload-images.jianshu.io/upload_images/1162126-0480536300e48f30.png?imageMogr2/auto-orient/strip%7CimageView2/2" alt=""><br><img src="http://upload-images.jianshu.io/upload_images/1162126-d159781539f2f376.png?imageMogr2/auto-orient/strip%7CimageView2/2" alt=""><br><img src="http://upload-images.jianshu.io/upload_images/1162126-1bb14ddbfd011292.png?imageMogr2/auto-orient/strip%7CimageView2/2" alt=""><br><img src="http://upload-images.jianshu.io/upload_images/1162126-3e160b39aebd8d00.png?imageMogr2/auto-orient/strip%7CimageView2/2" alt=""><br><img src="http://upload-images.jianshu.io/upload_images/1162126-d43e56fcb311b467.png?imageMogr2/auto-orient/strip%7CimageView2/2" alt=""></p>
<p>只要有了这三个RE基本操作的NFA,我们就能对任何组合得来的RE求出对应的NFA.</p>
<h2 id="词汇的词汇层分析："><a href="#词汇的词汇层分析：" class="headerlink" title="词汇的词汇层分析："></a>词汇的词汇层分析：</h2><ul>
<li>断词</li>
<li>形态分析</li>
</ul>
<p>不同语种任务不一样</p>
<p>ex：英语<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1.使用有限自动机进行英语的tokenization</div><div class="line">2.评价指标: F1指标：召回率和准确率的权衡 F1 = 2 * (precision * recall) / (precision + recall)</div></pre></td></tr></table></figure></p>
<p>英语形态分析 : 表层 =&gt; 词汇层，词汇层 =&gt; 表层<br>词形态分析需要的三个知识：</p>
<ul>
<li>lexicon： 词根list、词缀list</li>
<li>配列法规则：加词缀的次序</li>
<li>正字法规则：加词缀怎么表示 （如复数直接加s还是改y为i加es）</li>
</ul>
<p>表层 =&gt; 正字法规则 =&gt; 配列法规则 =&gt; 词汇层 （反过来顺序倒过来）</p>
<h2 id="算法模型设计"><a href="#算法模型设计" class="headerlink" title="算法模型设计"></a>算法模型设计</h2><p>1.词表（stem list, prefix list)词根表，词缀表可以使用 FSA 有限自动机来表示<br>2.配列规则、正字法规则 ： 应该使用 FST （因为规则需要由一个符号串输出另一个符号串，而不是FSA那样输出是否的判定）</p>
<p>FST: 串到串的映射</p>
<p>eg: fox + N + PL 通过一个实现配列规则的 FST生成fox^s#<br>    fox^s# 再通过一个实现正字法规则FST生成foxes# =&gt; 两个FST进行拼接就可以了</p>
<ul>
<li>最小编辑距离：动态规划求解（全局最优的局部一定是局部最优）左下到右上的递推过程<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dis[i][j] = min(min(dis[i-1][j] + ins(t[i]), dis[i-i][j-1] + sub(s[j]+t[i]), dis[i][j-1] + del(s[j])</div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://book.douban.com/subject/2403834/" target="_blank" rel="external">Speech and Language Processing</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/16/特征编码之Likelihood-Encoding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe Shaw">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shaw's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/09/16/特征编码之Likelihood-Encoding/" itemprop="url">
                  特征编码之Likelihood Encoding
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-16T10:45:47+08:00">
                2017-09-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h1><p>特征编码为特征工程中经常性的步骤，将类别变量编码成数值型变量，再扔给回归器、分类器处理。常见的编码方式有one-hot encoding、binary encoding等。之前进行构建矩阵、count_value、再将类别变量map的过程手动编码，现在有的模型已经可以自己处理编码（比如<a href="https://catboost.yandex/" target="_blank" rel="external">catboost</a>），但是很多时候模型自动编码的结果并不理想。</p>
<h2 id="One-Hot-Encoding"><a href="#One-Hot-Encoding" class="headerlink" title="One-Hot Encoding"></a>One-Hot Encoding</h2><p>简言之one就是将每个类别变量中的出现的值都列出来，然后在出现的对应位置上标1，其余位置标0。如下图所示：<br><img src="https://cdn-images-1.medium.com/max/600/1*3hdYEX5eixaV4F3wT5OmBg.png" alt=""><br>所以其特点是：</p>
<ul>
<li>编码后多增加的列数就是每个类别型变量的取值个数。</li>
<li>会有很多的0，是有少数的1。<br>所以当矩阵很稀疏的时候应该要考虑稀疏表示，但是很多机器学习模型都不是太支持稀疏存储（如lightgbm）。</li>
</ul>
<h2 id="One-Hot-Encoding-Performs-Poorly-in-Tree-Models"><a href="#One-Hot-Encoding-Performs-Poorly-in-Tree-Models" class="headerlink" title="One-Hot Encoding Performs Poorly in Tree Models"></a>One-Hot Encoding Performs Poorly in Tree Models</h2><p>在实际中做比赛时，会发现当类别变量的取值集合非常大时（成千上万个），那么使用one出来的矩阵是非常稀疏的，在集成树模型上的表现就会非常差。那么原因是什么呢？<br>可能原因是ohe后的特征空间很大，那么在sample一些特征作为当前该树用于分裂的特征时，很可能那些重要的特征没有被选上，或者说被选上的概率远远小于没有进行ohe时的概率。<br>所以对于这种取值集合很大的类别型变量，需要寻找其他编码方式。Likelihood Encoding就是其中一种。</p>
<h1 id="Likelihood-Encoding"><a href="#Likelihood-Encoding" class="headerlink" title="Likelihood Encoding"></a>Likelihood Encoding</h1><p>LE又称为Impacted Encoding。其思想主要是将类别型变量用对应的label变量取值的均值来代替。但是这样直接取均值的话，该类别变量就不存在与label直接的信息交换了，也就该变量的信息没能leak给label，所以会导致一些baised estimation。解决办法是使用 <strong><em>KFold</em></strong> 来交叉取值。而且一般取两层的CV。<br>具体过程如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">首先将数据集划分为20folds，然后使用fold #2-20来预测fold #1的值；</div><div class="line">将fold #2-20又️划分一层，划分为10 folds；</div><div class="line">计算这10 folds的out of folds值：比如使用fold #2-10 的对应label的均值，来作为fold #1的预测值；</div><div class="line">得到内层CV的10个均值，再将这10个取平均，则得到了外层fold #1的值；</div><div class="line">依次将外层20个fold都可以计算出来。</div></pre></td></tr></table></figure>
<h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import pandas as pd</div><div class="line">from sklearn.model_selection import KFold</div><div class="line">import dill as pickle</div><div class="line">import sys</div><div class="line"></div><div class="line">def input_data(train_file):</div><div class="line">    with open(train_file, &apos;rb&apos;) as f1:</div><div class="line">       train_data = pickle.load(f1)</div><div class="line">    cat_feature = []</div><div class="line">    for dtype, feature in zip(train_data.dtypes, train_data.columns):</div><div class="line">        if dtype == object:</div><div class="line">            cat_feature.append(feature)</div><div class="line">    print cat_feature</div><div class="line">    return (train_data, cat_feature)</div><div class="line"></div><div class="line">def clean_noise(data):</div><div class="line">    MinLogError = -0.4</div><div class="line">    MaxLogError = 0.418</div><div class="line">    data = data[(data[&apos;logerror&apos;] &gt; MinLogError) &amp; (data[&apos;logerror&apos;] &lt; MaxLogError)]</div><div class="line">    return data</div><div class="line"></div><div class="line">def likelihood_encoding(data, feature, data_type, target = &apos;logerror&apos;):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    :param data:</div><div class="line">    :param feature:</div><div class="line">    :param terget:</div><div class="line">    :return: likelihood encoded data</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    #data = data[f].values.astype(np.str_, copy=False)</div><div class="line">    np.random.seed(2017)</div><div class="line">    n_folds = 10</div><div class="line">    n_inner_folds = 5</div><div class="line">    likelihood_encoded = pd.Series()</div><div class="line">    ##global mean, could be tuned later</div><div class="line">    oof_default_mean = data[target].mean()</div><div class="line">    kf = KFold(n_splits=n_folds, shuffle=True)</div><div class="line">    oof_mean_cv = pd.DataFrame()</div><div class="line">    split = 0</div><div class="line">    print (&apos;raw data shape &#123;&#125;&apos;.format(data.shape))</div><div class="line"></div><div class="line">    for infold, oof in kf.split(data[feature]):</div><div class="line">        #print (&apos;infold data shape %s , oof data shape %s&apos;</div><div class="line">        #       % (data.iloc[infold].shape, data.iloc[oof].shape))</div><div class="line"></div><div class="line">        print (&apos;==============level 1 encoding..., fold %s ============&apos; % split)</div><div class="line">        inner_kf = KFold(n_splits=n_inner_folds, shuffle=True)</div><div class="line">        inner_oof_default_mean = data.iloc[infold][target].mean()</div><div class="line">        inner_split = 0</div><div class="line">        ## inner out of fold mean, used for outer oof</div><div class="line">        inner_oof_mean_cv = pd.DataFrame()</div><div class="line">        ##</div><div class="line">        likelihood_encoded_cv = pd.Series()</div><div class="line">        for inner_infold, inner_oof in inner_kf.split(data.iloc[infold]):</div><div class="line">            #print (&apos;innner infold data shape %s , inner oof data shape %s&apos;</div><div class="line">            #       % (data.iloc[inner_infold].shape, data.iloc[inner_oof].shape))</div><div class="line">            print (&apos;==============level 2 encoding..., inner fold %s ============&apos; % inner_split)</div><div class="line">            ## inner out of fold mean</div><div class="line">            oof_mean = data.iloc[inner_infold].groupby(by=feature)[target].mean()</div><div class="line">            # assign oof_mean to the in fold</div><div class="line">            likelihood_encoded_cv = likelihood_encoded_cv.append(data.iloc[infold].apply(</div><div class="line">                lambda x : oof_mean[x[feature]]</div><div class="line">                if x[feature] in oof_mean.index</div><div class="line">                else inner_oof_default_mean</div><div class="line">                , axis = 1</div><div class="line">            ))</div><div class="line">            inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how=&apos;outer&apos;)</div><div class="line"></div><div class="line">            inner_oof_mean_cv.fillna(inner_oof_default_mean, inplace=True)</div><div class="line">            inner_split += 1</div><div class="line"></div><div class="line">        #print inner_oof_mean_cv.head()</div><div class="line">        #print inner_oof_mean_cv.index</div><div class="line">        #sys.exit(1)</div><div class="line"></div><div class="line">        oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how=&apos;outer&apos;)</div><div class="line"></div><div class="line">        oof_mean_cv.fillna(value=oof_default_mean, inplace=True)</div><div class="line">        split += 1</div><div class="line">        print (&apos;============final mapping...===========&apos;)</div><div class="line"></div><div class="line">        #print type(data.iloc[1][feature][0]), &apos; &apos;, data.iloc[1][feature][0]</div><div class="line">        # print data.iloc[oof][feature]</div><div class="line">        # print(np.mean(inner_oof_mean_cv.ix[&apos;6037&apos;].values))</div><div class="line">        #sys.exit(1)</div><div class="line"></div><div class="line">        likelihood_encoded = likelihood_encoded.append(data.iloc[oof].apply(</div><div class="line">            lambda x: np.mean(inner_oof_mean_cv.ix[x[feature]].values)</div><div class="line">            if x[feature] in inner_oof_mean_cv.index</div><div class="line">            else oof_default_mean</div><div class="line">            , axis=1</div><div class="line">        ))</div><div class="line">    return (likelihood_encoded, oof_mean_cv.mean(axis = 1), oof_default_mean)</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    i_file = &apos;train.pkl&apos;</div><div class="line">    train_data, cat_feature = input_data(i_file)</div><div class="line">    clean_noise(train_data)</div><div class="line">    likelihood_coding_map = &#123;&#125;</div><div class="line"></div><div class="line">    debug_cat_feature = [&apos;fipsid&apos;, &apos;tractid&apos;]</div><div class="line">    # debug_cat_feature = [&apos;blockid&apos;]</div><div class="line"></div><div class="line">    for f in debug_cat_feature:</div><div class="line">        data_type = False</div><div class="line">        print (&apos;Likelihood coding for &#123;&#125;&apos;.format(f))</div><div class="line">        if type(train_data.loc[0][f]) == float:</div><div class="line">            data_type = True</div><div class="line">        train_data[f], likelihood_coding_mapping, default_coding = likelihood_encoding(train_data, f, data_type)</div><div class="line">        #likelihood_coding_map[f] = (likelihood_coding_mapping, default_coding)</div><div class="line">        # mapping, default_mean = likelihood_coding_map[f]</div><div class="line">        # test_data[f] = test_data.apply(lambda x : mapping[x[f]]</div><div class="line">        #                                if x[f] in mapping</div><div class="line">        #                                else default_mean</div><div class="line">        #                                ,axis = 1)</div><div class="line">    print train_data.head()</div><div class="line">    print &apos;=============================================&apos;</div><div class="line"></div><div class="line">    print train_data[&apos;fipsid&apos;].value_counts()</div><div class="line"></div><div class="line">    with open(&apos;encoded_train.pkl&apos;, &apos;wb&apos;) as f1:</div><div class="line">        pickle.dump(train_data, f1, -1)</div><div class="line">    f1.close()</div></pre></td></tr></table></figure>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931" target="_blank" rel="external">Visiting: Categorical Features and Encoding in Decision Trees</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/03/使用CNN进行网站文本分类/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe Shaw">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shaw's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/09/03/使用CNN进行网站文本分类/" itemprop="url">
                  使用CNN进行网站文本分类
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-03T20:59:08+08:00">
                2017-09-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要讲述一些自然语言处理的基础知识，以及使用如何CNN模型对从网上爬取下来的网站网页文本进行分类。</p>
<h1 id="Wod2Vec"><a href="#Wod2Vec" class="headerlink" title="Wod2Vec"></a>Wod2Vec</h1><p>词向量技术就是将词转化成向量形式，因为我们的分类器最后处理的都是向量，所以进行自然语言的预处理往往就是需要进行向量化。在sklearn库中使用svm对文本分类之前可以通过使用 <strong><em>CountVectorizer()</em></strong> 和 <strong><em>TfidfTransformer()</em></strong> 来向量化。这两个函数所进行的操作就是最基本的利用词频和反文档词频特征进行向量化，但当我们的词量很大的时候，这样构造出来的矩阵就会非常大（也许它内部进行了矩阵压缩等优化），并且，这样最基本的词语特征并不能反映更高级的语义特征，所以，我们需要寻求更有效的向量方法。</p>
<p>谷歌推出的<a href="https://zh.wikipedia.org/wiki/Word2vec" target="_blank" rel="external">Word2Vec</a>技术，就是很流行的方法。在给定一个中心词的情况下，Word2Vec 通过最大化上下文单词的对数概率并通过随机梯度下降（SGD）来修改向量，试图找到不同词的向量表征。因此它可以展现出不同词向量之间的线性关系，也就是通过神经网络的映射，可以扑捉到一些不同的语法和语义概念。</p>
<h1 id="CNN文本分类"><a href="#CNN文本分类" class="headerlink" title="CNN文本分类"></a>CNN文本分类</h1><p>使用CNN进行文本分类，是因为CNN的卷积操作能够提取一些重要的文本特征。实现主要参考了<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="external"> Convolutional Neural Networks for Sentence Classification.</a>这篇文章。</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>对于从网页上爬下来的文本首相要进行正文提取、去除特殊字符、分词、词语连接等操作。然后要进行文本的padding，也就是对齐，将向量统一维度。再放入Word2Vec模型里进行训练，然后得到词典，也就得到了每个词语的向量映射。<br>函数data_helper()就是帮助前期处理数据的。包括数据的读入，我们需要分的类别是12类，所以将label表示为一个12维的向量，对应的data是哪个类就将这个类置1，其余为0。</p>
<h2 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h2><p>这个模型只包含了一层卷积层和一个池化层。卷积有三种长度分别为[3, 4, 5]。<br>所以卷积核的大小应该是一个三维的向量：[卷积核大小， 嵌入的维度大小，卷积核的数量]，但是tensorflow里面规定的特卷积核大小还有个通道数，由于这里不是图像而是文本，就将通道数置为1。<br>然后再通过一个非线性函数、池化操作得到输出。最后会有一个reshape来将所有的特征连接在一起并延展成一维的向量，用于之后的softmax计算概率。</p>
<p>模型核心的代码入如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">ooled_outputs = []</div><div class="line">for i, filter_size in enumerate(filter_sizes):</div><div class="line">    with tf.name_scope(&quot;conv-maxpool-%s&quot; % filter_size):</div><div class="line">        # 卷积层</div><div class="line">        filter_shape = [filter_size, embedding_size, 1, num_filters]</div><div class="line">        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=&quot;W&quot;)</div><div class="line">        b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=&quot;b&quot;)</div><div class="line">        conv = tf.nn.conv2d(</div><div class="line">            self.embedded_chars_expanded,</div><div class="line">            W,</div><div class="line">            strides=[1, 1, 1, 1],</div><div class="line">            padding=&quot;VALID&quot;,</div><div class="line">            name=&quot;conv&quot;)</div><div class="line">        # 通过非线性函数</div><div class="line">        h = tf.nn.relu(tf.nn.bias_add(conv, b), name=&quot;relu&quot;)</div><div class="line">        # 池化操作</div><div class="line">        pooled = tf.nn.max_pool(</div><div class="line">            h,</div><div class="line">            ksize=[1, sequence_length - filter_size + 1, 1, 1],</div><div class="line">            strides=[1, 1, 1, 1],</div><div class="line">            padding=&apos;VALID&apos;,</div><div class="line">            name=&quot;pool&quot;)</div><div class="line">        pooled_outputs.append(pooled)</div><div class="line"></div><div class="line"># 连接所有pooled features</div><div class="line">num_filters_total = num_filters * len(filter_sizes)</div><div class="line">self.h_pool = tf.concat(3, pooled_outputs)</div><div class="line">self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])</div></pre></td></tr></table></figure>
<h2 id="防止过拟合"><a href="#防止过拟合" class="headerlink" title="防止过拟合"></a>防止过拟合</h2><p>为防止过拟合，可以加入正则化，和dropout等措施。dropout的概率默认值设为0.5，实际需要看训练过程的bias and variance trade-off来进行调整。正则化选择L2正则，因为模型本来并不太复杂，默认值为0，看实际情况进行调整。</p>
<h1 id="实际训练中遇到的问题"><a href="#实际训练中遇到的问题" class="headerlink" title="实际训练中遇到的问题"></a>实际训练中遇到的问题</h1><h2 id="Memory-Error"><a href="#Memory-Error" class="headerlink" title="Memory Error"></a>Memory Error</h2><p>开始时我将一共十多万条文本数据全部进行向量化后的向量转化成numpy形式的数组，执行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x_test = np.array(all_vectors)</div></pre></td></tr></table></figure></p>
<p>这一步会出现内存错误，st上说是因为 <strong><em>np.rarray</em></strong> 方法将数组整个一次性复制到内存，可能因为内存不够，但是我是放在服务器上跑的，有几十G的内存，x_test的大小应该是 <strong>文本条数 x 最大文本长度 x embedding_size</strong> 。检查之前的数据处理步骤，发现分词是直接单个字进行分词的，最大文本长度就会非常大，所以最后x_test向量会很大。之后我改成了基于词语的分词，然后还是发现内存错误。<br>st上说使用 <strong><em>np.asarray</em></strong> 方法，不会将数据一次性复制进内存。试了下就没有出现内存错误了。所以最后原因应该是因为文本长度太大了，embedding_size 这个值设置的是128，最后三个向量相乘的得到的矩阵维度会很大。<br>另外一个解决办法是在文本padding过程中需要将长度不够最大长度的文本补齐，所以万一短文本很多，最后的矩阵其实会有很多0值，是个很稀疏的矩阵，所以可以考虑使用稀疏矩阵存储的方式。</p>
<h2 id="过拟合严重"><a href="#过拟合严重" class="headerlink" title="过拟合严重"></a>过拟合严重</h2><p>训练时将训练数据进行1:0.8的划分，一部分训练，一部分验证。训练集模型正确率可以达到90%多，并且在验证集上表现和训练集曲线接近，相差不大。但是放到真实的测试数据（另一批没有标签的数据）上表现不是很好。最后原因可能是因为模型过拟合严重，因为训练数据是使用关键词匹配出来的（网页的类别和网页内容中的关键词），数据质量非常好。但是测试时拿的数据是另一批从网上爬去下来的数据，良莠不齐，模型表现并不理想。<br>尝试过提高dropout的概率，加入正则化等措施，效果不明显。应该是数据集的影响比较大。</p>
<h2 id="在不同类别上表现差异较大"><a href="#在不同类别上表现差异较大" class="headerlink" title="在不同类别上表现差异较大"></a>在不同类别上表现差异较大</h2><p>最后训练得到的模型在另一批数据上进行测试的时候发现12种类别，在不同类别上表现差异较大，并且并不是所有类别都能分出来。<br>最后发现原因是训练集里没有确保样本均衡，不同类别样本数相差较大。改进后在测试数据集上的不同类别上的变现更好。并且十二类都完全分的出来。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文记录了在实现CNN文本分类中遇到的一些问题，及解决方法。改进之处是还可以改进模型结构，提高泛化能力，比如可以进行数据增强(data augmentation)等措施，但是在自然语言领域data augmentation的方法比较困难，之后可以尝试一下。</p>
<p>相关代码已经方法github上，相比源来的代码改写了data_helper.py，增加了eval_helper.py，用于check 真实的测试数据的维度是否一致等。并且用于提取CNN中提取到的句子的特征，是一个 <strong>embedding_size x filer_num</strong> 的向量，将这个特征拿出来可以用于后续的任务。 <a href="https://github.com/ZoeShaw101/cnn_website_text_classify" target="_blank" rel="external">代码戳这里</a> 求star啊~</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/" target="_blank" rel="external">Implementing a CNN for Text Classification in TensorFlow</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/24758451" target="_blank" rel="external">用CNN做句子分类：CNN Sentence Classification (with Theano code)</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/21/Tensorflow学习笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe Shaw">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shaw's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/07/21/Tensorflow学习笔记/" itemprop="url">
                  Tensorflow学习笔记
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-21T15:19:00+08:00">
                2017-07-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Tensorflow是一个开源的深度学习框架，它的基本计算流程是需要首先定义好计算图，然后放入数据（张量）进行初始化，再使用会话步骤是数据沿着计算图流通，最后得到我们需要的计算结果。<br>本文主要讲解Tensorflow最基本的几个数据类型及其使用,并构建了基本的神经网络。</p>
<h1 id="Session-会话控制"><a href="#Session-会话控制" class="headerlink" title="Session 会话控制"></a>Session 会话控制</h1><p>session 是Tensorflow为了控制,和输出文件的执行的语句.创建了数据或变量并没有执行任何计算操作，只有使用sess.run()语句，才能进行运算。<br>代码解释：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#encoding:utf-8</div><div class="line">import tensorflow as tf</div><div class="line">matrix1 = tf.constant([[3,3]])</div><div class="line">matrix2 = tf.constant([[2],</div><div class="line">                       [2]])</div><div class="line">product = tf.matmul(matrix1,matrix2)</div></pre></td></tr></table></figure></p>
<p>这里并没有执行乘法操作哦，只是构建好了这个计算图的步骤。<br>接下来需要打开会话session，这里有两种方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">方法一：</div><div class="line">sess = tf.Session()</div><div class="line">result = sess.run(product)</div><div class="line">print(result)</div><div class="line">sess.close()</div><div class="line"></div><div class="line">方法二：</div><div class="line">with tf.Session() as sess:</div><div class="line">    result2 = sess.run(product)</div><div class="line">    print(result2)</div></pre></td></tr></table></figure></p>
<h1 id="Variable-变量"><a href="#Variable-变量" class="headerlink" title="Variable 变量"></a>Variable 变量</h1><p>在Tensorflow中，变量和常量是区分开的，就是说某个数定义了是变量，它才是变量。<br>变量是使用tf.Variable()，常量是使用tf.constant()</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">#encoding:utf-8</div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">state = tf.Variable(0, name = &apos;counter&apos;)</div><div class="line">one = tf.constant(1)</div><div class="line">new_value = tf.add(state, one) #注意！这步并没有直接计算值！</div><div class="line">update = tf.assign(state, new_value)</div><div class="line">#定义了变量还没激活，只有定义了session，使用session.run()才激活变量，才能计算值！</div><div class="line">init = tf.global_variables_initializer() #定义了变量就一定要初始化！</div><div class="line">with tf.Session() as sess:</div><div class="line">    sess.run(init)</div><div class="line">    for _ in range(3):</div><div class="line">        sess.run(state)</div><div class="line">        print sess.run(state)</div></pre></td></tr></table></figure>
<blockquote>
<p>注意：直接 print(state) 不起作用！！<br>一定要把 sess 的指针指向 state 再进行 print 才能得到想要的结果！</p>
</blockquote>
<h1 id="Placeholder-传入值"><a href="#Placeholder-传入值" class="headerlink" title="Placeholder 传入值"></a>Placeholder 传入值</h1><p>placeholder 是 Tensorflow 中的占位符，暂时储存变量.<br>Tensorflow 如果想要从外部传入data, 那就需要用到 <strong><em>tf.placeholder()</em></strong>, 然后以这种形式传输数据 <strong><em>sess.run(…, feed_dict={input: ..})</em></strong>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line"></div><div class="line">#在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式</div><div class="line">input1 = tf.placeholder(tf.float32)</div><div class="line">input2 = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line"># mul = multiply 是将input1和input2 做乘法运算，并输出为 output</div><div class="line">ouput = tf.multiply(input1, input2)</div></pre></td></tr></table></figure></p>
<p>接下来, 传值的工作交给了 sess.run() , 需要传入的值放在了feed_dict={} 并一一对应每一个 input, placeholder 与 feed_dict={} 是绑定在一起出现的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">with tf.Session() as sess:</div><div class="line">    print(sess.run(ouput, feed_dict=&#123;input1: [7.], input2: [2.]&#125;))</div></pre></td></tr></table></figure></p>
<h1 id="构建一个两层的神经网络"><a href="#构建一个两层的神经网络" class="headerlink" title="构建一个两层的神经网络"></a>构建一个两层的神经网络</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">#encoding:utf-8</div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">def add_layer(inputs, in_size, out_size, activation_function = None):</div><div class="line">    # type: (object, object, object, object) -&gt; object</div><div class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</div><div class="line">    bias = tf.Variable(tf.zeros([1, out_size]) + 0.1)</div><div class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + bias</div><div class="line">    if  activation_function == None:</div><div class="line">        output = Wx_plus_b</div><div class="line">    else:</div><div class="line">        output = activation_function(Wx_plus_b)</div><div class="line">    return output</div><div class="line"></div><div class="line">x_data = np.linspace(-1, 1, 300, dtype=np.float32)[:, np.newaxis]</div><div class="line">noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)</div><div class="line">y = np.square(x_data) + 0.5 + noise</div><div class="line"></div><div class="line">#利用占位符定义我们所需的神经网络的输入</div><div class="line"></div><div class="line">xs = tf.placeholder(tf.float32, [None, 1])</div><div class="line">ys = tf.placeholder(tf.float32, [None, 1])</div><div class="line"></div><div class="line"></div><div class="line">l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)</div><div class="line">predict = add_layer(l1, 10, 1, activation_function=None)</div><div class="line"></div><div class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(predict - y), reduction_indices=[1]))</div><div class="line"></div><div class="line">train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)</div><div class="line"></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div><div class="line"></div><div class="line">for i in range(50000):</div><div class="line">    sess.run(train_step, feed_dict=&#123;xs : x_data, ys : y&#125;)</div><div class="line">    if i % 10 == 0:</div></pre></td></tr></table></figure>
<h1 id="构建RNN-LSTM回归模型"><a href="#构建RNN-LSTM回归模型" class="headerlink" title="构建RNN LSTM回归模型"></a>构建RNN LSTM回归模型</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line">BATCH_START = 0     # 建立 batch data 时候的 index</div><div class="line">TIME_STEPS = 20     # backpropagation through time 的 time_steps</div><div class="line">BATCH_SIZE = 50     </div><div class="line">INPUT_SIZE = 1      # sin 数据输入 size</div><div class="line">OUTPUT_SIZE = 1     # cos 数据输出 size</div><div class="line">CELL_SIZE = 10      # RNN 的 hidden unit size</div><div class="line">LR = 0.006          # learning rate</div><div class="line"></div><div class="line">def get_batch():</div><div class="line">    global BATCH_START, TIME_STEPS</div><div class="line">    # xs shape (50batch, 20steps)</div><div class="line">    xs = np.arange(BATCH_START, BATCH_START+TIME_STEPS*BATCH_SIZE).reshape((BATCH_SIZE, TIME_STEPS)) / (10*np.pi)</div><div class="line">    seq = np.sin(xs)</div><div class="line">    res = np.cos(xs)</div><div class="line">    BATCH_START += TIME_STEPS</div><div class="line">    # returned seq, res and xs: shape (batch, step, input)</div><div class="line">    return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]</div></pre></td></tr></table></figure>
<p>将模型定义成一个类，方便数据的传入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">class LSTMRNN(object):</div><div class="line">    def __init__(self, n_steps, input_size, output_size, cell_size,    batch_size):</div><div class="line">        self.n_steps = n_steps</div><div class="line">        self.input_size = input_size</div><div class="line">        self.output_size = output_size</div><div class="line">        self.cell_size = cell_size</div><div class="line">        self.batch_size = batch_size</div><div class="line">        with tf.name_scope(&apos;inputs&apos;):</div><div class="line">            self.xs = tf.placeholder(tf.float32, [None, n_steps, input_size], name=&apos;xs&apos;)</div><div class="line">            self.ys = tf.placeholder(tf.float32, [None, n_steps, output_size], name=&apos;ys&apos;)</div><div class="line">        with tf.variable_scope(&apos;in_hidden&apos;):</div><div class="line">            self.add_input_layer()</div><div class="line">        with tf.variable_scope(&apos;LSTM_cell&apos;):</div><div class="line">            self.add_cell()</div><div class="line">        with tf.variable_scope(&apos;out_hidden&apos;):</div><div class="line">            self.add_output_layer()</div><div class="line">        with tf.name_scope(&apos;cost&apos;):</div><div class="line">            self.compute_cost()</div><div class="line">        with tf.name_scope(&apos;train&apos;):</div><div class="line">            self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost)</div></pre></td></tr></table></figure>
<p>添加输入层，门单元，输出层：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def add_input_layer(self,):</div><div class="line">        l_in_x = tf.reshape(self.xs, [-1, self.input_size], name=&apos;2_2D&apos;)  # (batch*n_step, in_size)</div><div class="line">        # Ws (in_size, cell_size)</div><div class="line">        Ws_in = self._weight_variable([self.input_size, self.cell_size])</div><div class="line">        # bs (cell_size, )</div><div class="line">        bs_in = self._bias_variable([self.cell_size,])</div><div class="line">        # l_in_y = (batch * n_steps, cell_size)</div><div class="line">        with tf.name_scope(&apos;Wx_plus_b&apos;):</div><div class="line">            l_in_y = tf.matmul(l_in_x, Ws_in) + bs_in</div><div class="line">        # reshape l_in_y ==&gt; (batch, n_steps, cell_size)</div><div class="line">        self.l_in_y = tf.reshape(l_in_y, [-1, self.n_steps, self.cell_size], name=&apos;2_3D&apos;)</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def add_cell(self):</div><div class="line">        lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True)</div><div class="line">        with tf.name_scope(&apos;initial_state&apos;):</div><div class="line">            self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)</div><div class="line">        self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(</div><div class="line">            lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=False)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def add_output_layer(self):</div><div class="line">        # shape = (batch * steps, cell_size)</div><div class="line">        l_out_x = tf.reshape(self.cell_outputs, [-1, self.cell_size], name=&apos;2_2D&apos;)</div><div class="line">        Ws_out = self._weight_variable([self.cell_size, self.output_size])</div><div class="line">        bs_out = self._bias_variable([self.output_size, ])</div><div class="line">        # shape = (batch * steps, output_size)</div><div class="line">        with tf.name_scope(&apos;Wx_plus_b&apos;):</div><div class="line">            self.pred = tf.matmul(l_out_x, Ws_out) + bs_out</div></pre></td></tr></table></figure>
<p>模型定义完成，损失函数的计算：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">def compute_cost(self):</div><div class="line">        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(</div><div class="line">            [tf.reshape(self.pred, [-1], name=&apos;reshape_pred&apos;)],</div><div class="line">            [tf.reshape(self.ys, [-1], name=&apos;reshape_target&apos;)],</div><div class="line">            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],</div><div class="line">            average_across_timesteps=True,</div><div class="line">            softmax_loss_function=self.ms_error,</div><div class="line">            name=&apos;losses&apos;</div><div class="line">        )</div><div class="line">        with tf.name_scope(&apos;average_cost&apos;):</div><div class="line">            self.cost = tf.div(</div><div class="line">                tf.reduce_sum(losses, name=&apos;losses_sum&apos;),</div><div class="line">                self.batch_size,</div><div class="line">                name=&apos;average_cost&apos;)</div><div class="line">            tf.summary.scalar(&apos;cost&apos;, self.cost)</div></pre></td></tr></table></figure>
<p>均方误差和权值，偏置的设置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def ms_error(self, y_target, y_pre):</div><div class="line">    return tf.square(tf.sub(y_target, y_pre))</div><div class="line"></div><div class="line">def _weight_variable(self, shape, name=&apos;weights&apos;):</div><div class="line">    initializer = tf.random_normal_initializer(mean=0., stddev=1.,)</div><div class="line">    return tf.get_variable(shape=shape, initializer=initializer, name=name)</div><div class="line"></div><div class="line">def _bias_variable(self, shape, name=&apos;biases&apos;):</div><div class="line">    initializer = tf.constant_initializer(0.1)</div><div class="line">    return tf.get_variable(name=name, shape=shape, initializer=initializer)</div></pre></td></tr></table></figure></p>
<h2 id="训练-LSTM-RNN"><a href="#训练-LSTM-RNN" class="headerlink" title="训练 LSTM RNN"></a>训练 LSTM RNN</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    # 搭建 LSTMRNN 模型</div><div class="line">    model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)</div><div class="line">    sess = tf.Session()</div><div class="line">    # sess.run(tf.initialize_all_variables()) # tf 马上就要废弃这种写法</div><div class="line">    # 替换成下面的写法:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line">    # 训练 200 次</div><div class="line">    for i in range(200):</div><div class="line">        seq, res, xs = get_batch()  # 提取 batch data</div><div class="line">        if i == 0:</div><div class="line">        # 初始化 data</div><div class="line">            feed_dict = &#123;</div><div class="line">                    model.xs: seq,</div><div class="line">                    model.ys: res,</div><div class="line">            &#125;</div><div class="line">        else:</div><div class="line">            feed_dict = &#123;</div><div class="line">                model.xs: seq,</div><div class="line">                model.ys: res,</div><div class="line">                model.cell_init_state: state    # 保持 state 的连续性</div><div class="line">            &#125;</div><div class="line"></div><div class="line">        # 训练</div><div class="line">        _, cost, state, pred = sess.run(</div><div class="line">            [model.train_op, model.cost, model.cell_final_state, model.pred],</div><div class="line">            feed_dict=feed_dict)</div><div class="line"></div><div class="line">        # 打印 cost 结果</div><div class="line">        if i % 20 == 0:</div><div class="line">            print(&apos;cost: &apos;, round(cost, 4))</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/06/机器学习算法之集成学习boosting/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe Shaw">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shaw's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/07/06/机器学习算法之集成学习boosting/" itemprop="url">
                  机器学习算法之集成学习boosting
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-06T12:06:41+08:00">
                2017-07-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><br></script></p>
<h1 id="一、Adaboost"><a href="#一、Adaboost" class="headerlink" title="一、Adaboost"></a>一、Adaboost</h1><ul>
<li>关键思想：迭代增强 =&gt; 将一系列弱学习器，通过改变训练数据集的概率分布（权值）来迭代生成强学习器的方法。</li>
<li><p>算法流程：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"> 1.初始化样本权值分布：均匀的权值分布</div><div class="line"> 2.利用初始样本分布得到初始基分类器</div><div class="line"> 3.迭代更新，直到达到训练轮次</div><div class="line">    计算当前分类器在数据集上的分类错误率；</div><div class="line">    =&gt; 当前分类器权值要使分类错误率最小（最小化指数损失函数）：</div><div class="line">    =&gt; 求导得到分类器权值</div><div class="line">    =&gt; 更新数据集的权值分布</div><div class="line">4.最后得到集成模型</div></pre></td></tr></table></figure>
<p>更新训练数据集的权值分布，能使得被错误分类的数据在下一轮能得到更大的关注。<br>这里给出的算法的损失函数是指数函数，是针对分类问题；对于回归问题，可使用平方误差损失函数。<br>整个优化过程是前向分布迭代与开发模型的结合。</p>
</li>
</ul>
<h1 id="二、boosting-tree"><a href="#二、boosting-tree" class="headerlink" title="二、boosting tree"></a>二、boosting tree</h1><p>  基学习器是决策树。可使用分类树或回归树（只是损失函数不同）</p>
<ul>
<li>基本思想：对于回归树，损失函数为平方函数，损失其实就变为数据的残差。实质上是利用模型拟合残差。</li>
<li>算法流程：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">1.初始化回归树模型</div><div class="line">2.迭代更新，直到达到训练轮次</div><div class="line">   计算残差</div><div class="line">   =&gt; 拟合残差学习一个回归树</div><div class="line">   =&gt; 更新当前回归树模型</div><div class="line">3.最后得到集成回归树模型</div></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="三、GBDT-Gradient-Boosting-Decision-Tree"><a href="#三、GBDT-Gradient-Boosting-Decision-Tree" class="headerlink" title="三、GBDT : Gradient Boosting Decision Tree"></a>三、GBDT : Gradient Boosting Decision Tree</h1><h2 id="1-梯度提升"><a href="#1-梯度提升" class="headerlink" title="1. 梯度提升"></a>1. 梯度提升</h2><p>对于一般的损失函数(如绝对值损失函数），每一步优化不如平方损失和指数损失那样简单，这时候就要计算损失函数的负梯度，来近似拟合残差。残差其实可以看作负梯度的特例。</p>
<h2 id="2-回归树"><a href="#2-回归树" class="headerlink" title="2.回归树"></a>2.回归树</h2><p>回归树的总体流程与分类树类似，仍然需要特征选择、树的生成、剪枝优化。但是回归树的与分类树有两点不同：</p>
<blockquote>
<ol>
<li>每个节点都会有一个预测值 : 这个预测值往往是所有属于这个节点的值的均值（下面有解释）</li>
<li>划分节点时采用的是最小均方差，而不是最大熵值</li>
</ol>
</blockquote>
<p>实际上将回归树和分类树的构建流程类似，可以统称这类算法为分类与回归树(classification and regression tree, CART)。决策树的构建过程就是递归地建立决策树的过程，回归树使用最小均方差，而分类树使用最大熵值。</p>
<p><strong>问题一：为什么每个节点都会有一个预测值是所有属于这个节点的值的均值？</strong></p>
<p>因为整个回归树的构建实际上就是不断对特征空间进行划分，以及在每个划分单元上都具有输出值。使用最小平方误差:<br>                                 $$L(\theta) = \sum_i (y_i-\hat{y}_i)^2$$<br>使得这个损失函数最小，则最优的预测值应该为所有属于这个划分单元的值的均值。</p>
<p><strong>问题二：怎样进行特征空间的划分？</strong><br>采用贪心的方法，先人选定一个切分变量j和切分点s，这样特征空间就被分成了两个区域。然后对于这两个区域的损失函数都应达到最小，且它们的和也最小。然后遍历所有输入变量，找到最优值。就可以构成最优切分对(j,s)。</p>
<ul>
<li>算法流程：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">1.选择最优切分变量：对初始输入的j遍历所有s，使得损失函数最小</div><div class="line">2.使用选定的对(j,s)划分区域，得到每个区域的输出值</div><div class="line">3.继续对两个区域调用1，2步，直到满足停止条件</div><div class="line">4.得到最终回归树</div></pre></td></tr></table></figure>
</li>
</ul>
<p>模型可以表示为：<br>$$\hat{y}<em>i = \sum</em>{k=1}^K f_k(x_i), f_k \in \mathcal{F}$$</p>
<h1 id="四、XGBoost：Extreme-Gradient-Boosting"><a href="#四、XGBoost：Extreme-Gradient-Boosting" class="headerlink" title="四、XGBoost：Extreme Gradient Boosting"></a>四、XGBoost：Extreme Gradient Boosting</h1><p>由以上介绍可看出随机森里和boosting tree的模型几乎没有差别，但两者的区别在于train的方法。</p>
<h2 id="1-训练过程"><a href="#1-训练过程" class="headerlink" title="1.训练过程"></a>1.训练过程</h2><p>所有监督学习的train的过程都可以看作是优化目标函数的过程。目标函数包括损失函数和正则化项。<br>$$\text{obj}(\theta) = \sum_i^n l(y_i, \hat{y}<em>i) + \sum</em>{k=1}^K \Omega(f_k)$$</p>
<p>迭代更新参数以最小化目标函数：那么boosting tree需要学习的参数是什么呢？<br>训练的过程可以视为每次加入一个新的树并且修正所学到的模型。xgboost算法的步骤和GB基本相同，都是首先初始化为一个常数，gb是根据一阶导数ri，xgboost是根据一阶导数gi和二阶导数hi，迭代生成基学习器，相加更新学习器。</p>
<h2 id="2-XGBoost的优化"><a href="#2-XGBoost的优化" class="headerlink" title="2.XGBoost的优化"></a>2.XGBoost的优化</h2><p>XGBoost之所以成为数据挖掘中常用到的算法，因为他在实现时进行了很多优化处理。</p>
<ul>
<li>xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率</li>
<li>在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。</li>
<li>xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率</li>
</ul>
<p>参考资料：<br>1.李航《统计学习方法》<br>2.周志华《机器学习》<br>3.<a href="http://www.cnblogs.com/wxquare/p/5541414.html" target="_blank" rel="external">一步一步理解GB、GBDT、xgboost</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/26/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe Shaw">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shaw's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/26/hello-world/" itemprop="url">
                  每个人择食而亡
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-26T20:08:29+08:00">
                2017-04-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>“生活以线路／织一张网／每个人择食而亡”<br>                                           ——— 陈鸿宇 《来信》</p>
</blockquote>
<p>最初听到这句歌词时，我脑海中就浮现了《没有人知道的大冒险》游戏中的西西弗斯推巨石的场景。永无休止地毫无意义的劳作，和耳旁呼啸而过的山谷的风声，西西弗斯却自认为发现了意义，他觉得自己是幸福的。</p>
<p>这个故事出自加缪的<a href="https://zh.wikipedia.org/wiki/%E8%A5%BF%E7%BB%AA%E5%BC%97%E6%96%AF%E7%A5%9E%E8%AF%9D" target="_blank" rel="external">The Myth of Sisyphus</a>，在这个短篇中，加缪提出了著名的“荒诞主义”哲学：上帝既不存在，世界也没有意义。</p>
<p>即使很让人失望，我们也不得不承认，整个宇宙就是个盲目而没有意义的过程，充满了各种杂音。我们只是暂时居住在一隅的过客，存在不过很短一段时间。但是如果你对别人说：人生是毫无意义的！别人肯定会觉得你很悲观消极，然后就会列出很多有意义的事，劝你看开点，人生还是很精彩的。是的，人类并不能接受没有意义这个设定，宁愿相信我们做的事都是有着非凡的意义。作出详细周密的计划，列出大大小小的目标，也为我们未来做着各种准备。也许我们毫无选择，但事实就是如此，我们确实只能这么做，但终究说来，这一些都毫无意义。</p>
<p>所以加缪认为，人生就像西西弗斯推巨石一样，是一场毫无意义的过程，没有用处，也不会成功，根本就无所谓成功可言。</p>
<p>然而，虽然本质上是如此，现实中确很少有人相信这样的真理。因为人类是聪明的，他们会自己创造“意义的网”。</p>
<p>我曾经以为，人类和动物的区别，当然是人类能主动思考的能力，自由意志之类，动物却没有任何思维。但是，最近这个观点被很多实验事实打破，比如2012年的<a href="http://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf" target="_blank" rel="external">剑桥意识宣言</a>中提到动物也具有构成意识的神经系统。那么人类之所以能成为万物之主，原因究竟是什么呢？</p>
<p>在尤瓦尔·赫拉利的<a href="https://book.douban.com/subject/26943161/" target="_blank" rel="external">未来简史</a>中提出（强烈安利这本书，另外最近还在读《人类简史》，同样安利），人类能够区别于大猩猩统治地球，是因为人类能够大规模合作。并且是可以和无数陌生的个体灵活高效地合作。人类中最聪明、最具有领导能力的那个人就想，怎么才能更高效第合作呢？我们得制定规则、制度啊。这样一来，便编造出了一张意义的网：国家、公司、金钱、法律……我们更希望自己的生命有客观意义，希望自己的牺牲并不是为了脑子里的空想。我可以为国家战斗至死，可以为公司业绩拼命工作，也可以单纯地追求金钱的力量，或者坚决捍卫民主的法律。领导者们当然也需要民众相信这样的意义，这样社会才能更稳定和谐，更高效地发展。</p>
<p>所以，当人类编造出共同的“故事网”时，意义便诞生了。对我来说，并不会care在教堂结婚、斋戒月禁食和选举投票，但是在地球上另一些地方的人们，正是靠这些，才得以生生不息，繁荣生长。他们共同相信这些美好的意义，正如你不会对一个虔诚的基督教徒说：上帝已死。</p>
<p>原来，大多数人生活的意义，都只存在于彼此的叙述当中。</p>
<p>说了这么多，人生还不是毫无意义的？人生是荒谬的。那么，我们该跟到绝望，悲观吗？不，加缪说，认识到人生的荒谬性，就认识到了自由。</p>
<p>他认为，正是这样的荒谬性，正是这样的毫无意义，才让一切的所有都变得无所谓，我们具有了反抗这种荒谬性的自由。尽管这种反抗没有希望，但没有希望并不等于绝望，没有什么好在意的也就是没有什么好失去的。这个观点有几分超脱的味道：我坦然接受了人生的荒谬，但我有反抗这种荒谬的自由，尽管这反抗也注定失败，但这都是我自己的事，“我的命运属于我自己”。在没有上帝的世界里，不存在任何计划，除非我自己打造一个。</p>
<p>人要当自己的主宰者，要充分享受自己的一切面貌，积极把此刻过的更好。这让加缪觉得是幸福的。就像西西弗斯一样，在荒谬中发现了幸福。</p>
<p>这个观点至少是能给人带来一些慰藉了，但是，现代社会发展到今天，加缪没有想到的是，自由意志的神话也将被打破，你认为你自己主宰了自己的命运，也只是让你“以为”的“以为”。</p>
<p>《思考，快与慢》的作者丹尼尔·卡尼曼做过一个关于峰终定律的<a href="http://www.jianshu.com/p/16cfcd39be7b" target="_blank" rel="external">实验</a>：</p>
<blockquote>
<p>每个实验对象被告知该实验共有三个阶段。阶段1，实验对象将手伸入到14℃的冷水中60秒，此时人体感觉痛苦，但并非无法忍受； 阶段2，前60秒和阶段1一致，但在后30秒中，研究者打开温水阀门使水温升高1度左右，此时人体恰好能感觉到疼痛轻微降低。每个阶段间隔7分钟。阶段3，实验对象被要求从60秒短冷水实验或90秒长冷水实验中选择一项，再进行一次实验。</p>
</blockquote>
<p>实验对象在长冷水实验前60秒体验了和短冷水实验同样强度同等时间的痛苦，并且还需在30秒感受稍弱的疼痛感。实验结果显示80%的人选择了长实验感觉更好。<br>这个实验说明了人体有两个自我：体验自我（experiencing self）和叙事自我（narrating self 也叫 remembering self)，相关<a href="https://www.ted.com/talks/daniel_kahneman_the_riddle_of_experience_vs_memory" target="_blank" rel="external">ted介绍</a>。</p>
<p>生活中大多数关键抉择，都是由叙事自我决定的。尤瓦尔·赫拉利就认为，“自我”也如同国家、公司、金钱一样，都是虚构的事物：每个人都是一个复杂的系统，正如峰终定律一样，会将大部分的体验都抛弃，而去选择我们宁愿相信的那部分“自我”，告诉我们自己是谁、从哪里来、该到哪里去。当然这个虚构不是别人设定的，正是你自己无形中给自己设定的。</p>
<p>所以连“自我”都不存在，何来的自由选择自己的命运呢？究竟是什么在设定这这一切呢？</p>
<p>尤瓦尔·赫拉利的书中说到，人就是生物算法的集合，根本就不存在自由意志，我们平时所说的自由选择，可能是生物预设，也可能是各种生物算法的随机组合，但绝不是自由意志。人的欲望不是一种“选择”，而是我们感觉到了欲望，再据以行事。所以，有时候我们会感觉到算法似乎比我们更了解我们，比如安吉丽娜·朱莉做基因检测，最后发现了乳腺癌风险。再比如互联网的推荐系统，现在很多做得还不尽人意，但在可预想的未来，公投日那天，在你决定你想投哪个选举人之前，算法都已经根据你以往面对选举人时各种情感表现，以及你的认知心理更偏向的政党，早早就已经帮你做下了决定。</p>
<p>人类总是说不要被算法支配，但是人类本身就是算法的集合啊。</p>
<p>最后，说了这么多废话，每个人依然会各自择食而亡。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Zoe Shaw" />
          <p class="site-author-name" itemprop="name">Zoe Shaw</p>
           
              <p class="site-description motion-element" itemprop="description">Every hero has a code.</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/ZoeShaw101" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/ZoeShaw101" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zoe Shaw</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  





  






  





  

  

  

  

</body>
</html>
