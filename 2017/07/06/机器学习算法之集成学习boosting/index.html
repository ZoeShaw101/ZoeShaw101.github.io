<!DOCTYPE html>
<html lang="null">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>机器学习算法之集成学习boosting | Shaw&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一、Adaboost
关键思想：迭代增强 =&amp;gt; 将一系列弱学习器，通过改变训练数据集的概率分布（权值）来迭代生成强学习器的方法。
算法流程：
  12345678 1.初始化样本权值分布：均匀的权值分布 2.利用初始样本分布得到初始基分类器 3.迭代更新，直到达到训练轮次    计算当前分类器在数据集上的分类错误率；    =&amp;gt; 当前分类器权值要使分类错误率最小（最小化指数损失函数）">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习算法之集成学习boosting">
<meta property="og:url" content="http://yoursite.com/2017/07/06/机器学习算法之集成学习boosting/index.html">
<meta property="og:site_name" content="Shaw's Blog">
<meta property="og:description" content="一、Adaboost
关键思想：迭代增强 =&amp;gt; 将一系列弱学习器，通过改变训练数据集的概率分布（权值）来迭代生成强学习器的方法。
算法流程：
  12345678 1.初始化样本权值分布：均匀的权值分布 2.利用初始样本分布得到初始基分类器 3.迭代更新，直到达到训练轮次    计算当前分类器在数据集上的分类错误率；    =&amp;gt; 当前分类器权值要使分类错误率最小（最小化指数损失函数）">
<meta property="og:updated_time" content="2017-07-10T07:09:16.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习算法之集成学习boosting">
<meta name="twitter:description" content="一、Adaboost
关键思想：迭代增强 =&amp;gt; 将一系列弱学习器，通过改变训练数据集的概率分布（权值）来迭代生成强学习器的方法。
算法流程：
  12345678 1.初始化样本权值分布：均匀的权值分布 2.利用初始样本分布得到初始基分类器 3.迭代更新，直到达到训练轮次    计算当前分类器在数据集上的分类错误率；    =&amp;gt; 当前分类器权值要使分类错误率最小（最小化指数损失函数）">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/typing.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>

  
    
      <body>
    
  
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container" class="container">
        <article id="post-机器学习算法之集成学习boosting" class="article article-type-post" itemscope itemprop="blogPost">
  <header id="header" class="header">
  <nav id="main-nav" class="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
</header>

  <hr/>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习算法之集成学习boosting
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><br></script></p>
<h1 id="一、Adaboost"><a href="#一、Adaboost" class="headerlink" title="一、Adaboost"></a>一、Adaboost</h1><ul>
<li>关键思想：迭代增强 =&gt; 将一系列弱学习器，通过改变训练数据集的概率分布（权值）来迭代生成强学习器的方法。</li>
<li><p>算法流程：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"> 1.初始化样本权值分布：均匀的权值分布</div><div class="line"> 2.利用初始样本分布得到初始基分类器</div><div class="line"> 3.迭代更新，直到达到训练轮次</div><div class="line">    计算当前分类器在数据集上的分类错误率；</div><div class="line">    =&gt; 当前分类器权值要使分类错误率最小（最小化指数损失函数）：</div><div class="line">    =&gt; 求导得到分类器权值</div><div class="line">    =&gt; 更新数据集的权值分布</div><div class="line">4.最后得到集成模型</div></pre></td></tr></table></figure>
<p>更新训练数据集的权值分布，能使得被错误分类的数据在下一轮能得到更大的关注。<br>这里给出的算法的损失函数是指数函数，是针对分类问题；对于回归问题，可使用平方误差损失函数。<br>整个优化过程是前向分布迭代与开发模型的结合。</p>
</li>
</ul>
<h1 id="二、boosting-tree"><a href="#二、boosting-tree" class="headerlink" title="二、boosting tree"></a>二、boosting tree</h1><p>  基学习器是决策树。可使用分类树或回归树（只是损失函数不同）</p>
<ul>
<li>基本思想：对于回归树，损失函数为平方函数，损失其实就变为数据的残差。实质上是利用模型拟合残差。</li>
<li>算法流程：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">1.初始化回归树模型</div><div class="line">2.迭代更新，直到达到训练轮次</div><div class="line">   计算残差</div><div class="line">   =&gt; 拟合残差学习一个回归树</div><div class="line">   =&gt; 更新当前回归树模型</div><div class="line">3.最后得到集成回归树模型</div></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="三、GBDT-Gradient-Boosting-Decision-Tree"><a href="#三、GBDT-Gradient-Boosting-Decision-Tree" class="headerlink" title="三、GBDT : Gradient Boosting Decision Tree"></a>三、GBDT : Gradient Boosting Decision Tree</h1><h2 id="1-梯度提升"><a href="#1-梯度提升" class="headerlink" title="1. 梯度提升"></a>1. 梯度提升</h2><p>对于一般的损失函数(如绝对值损失函数），每一步优化不如平方损失和指数损失那样简单，这时候就要计算损失函数的负梯度，来近似拟合残差。残差其实可以看作负梯度的特例。</p>
<h2 id="2-回归树"><a href="#2-回归树" class="headerlink" title="2.回归树"></a>2.回归树</h2><p>回归树的总体流程与分类树类似，仍然需要特征选择、树的生成、剪枝优化。但是回归树的与分类树有两点不同：</p>
<blockquote>
<ol>
<li>每个节点都会有一个预测值 : 这个预测值往往是所有属于这个节点的值的均值（下面有解释）</li>
<li>划分节点时采用的是最小均方差，而不是最大熵值</li>
</ol>
</blockquote>
<p>实际上将回归树和分类树的构建流程类似，可以统称这类算法为分类与回归树(classification and regression tree, CART)。决策树的构建过程就是递归地建立决策树的过程，回归树使用最小均方差，而分类树使用最大熵值。</p>
<p><strong>问题一：为什么每个节点都会有一个预测值是所有属于这个节点的值的均值？</strong></p>
<p>因为整个回归树的构建实际上就是不断对特征空间进行划分，以及在每个划分单元上都具有输出值。使用最小平方误差:<br>                                 $$L(\theta) = \sum_i (y_i-\hat{y}_i)^2$$<br>使得这个损失函数最小，则最优的预测值应该为所有属于这个划分单元的值的均值。</p>
<p><strong>问题二：怎样进行特征空间的划分？</strong><br>采用贪心的方法，先人选定一个切分变量j和切分点s，这样特征空间就被分成了两个区域。然后对于这两个区域的损失函数都应达到最小，且它们的和也最小。然后遍历所有输入变量，找到最优值。就可以构成最优切分对(j,s)。</p>
<ul>
<li>算法流程：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">1.选择最优切分变量：对初始输入的j遍历所有s，使得损失函数最小</div><div class="line">2.使用选定的对(j,s)划分区域，得到每个区域的输出值</div><div class="line">3.继续对两个区域调用1，2步，直到满足停止条件</div><div class="line">4.得到最终回归树</div></pre></td></tr></table></figure>
</li>
</ul>
<p>模型可以表示为：<br>$$\hat{y}<em>i = \sum</em>{k=1}^K f_k(x_i), f_k \in \mathcal{F}$$</p>
<h1 id="四、XGBoost：Extreme-Gradient-Boosting"><a href="#四、XGBoost：Extreme-Gradient-Boosting" class="headerlink" title="四、XGBoost：Extreme Gradient Boosting"></a>四、XGBoost：Extreme Gradient Boosting</h1><p>由以上介绍可看出随机森里和boosting tree的模型几乎没有差别，但两者的区别在于train的方法。</p>
<h2 id="1-训练过程"><a href="#1-训练过程" class="headerlink" title="1.训练过程"></a>1.训练过程</h2><p>所有监督学习的train的过程都可以看作是优化目标函数的过程。目标函数包括损失函数和正则化项。<br>$$\text{obj}(\theta) = \sum_i^n l(y_i, \hat{y}<em>i) + \sum</em>{k=1}^K \Omega(f_k)$$</p>
<p>迭代更新参数以最小化目标函数：那么boosting tree需要学习的参数是什么呢？<br>训练的过程可以视为每次加入一个新的树并且修正所学到的模型。xgboost算法的步骤和GB基本相同，都是首先初始化为一个常数，gb是根据一阶导数ri，xgboost是根据一阶导数gi和二阶导数hi，迭代生成基学习器，相加更新学习器。</p>
<h2 id="2-XGBoost的优化"><a href="#2-XGBoost的优化" class="headerlink" title="2.XGBoost的优化"></a>2.XGBoost的优化</h2><p>XGBoost之所以成为数据挖掘中常用到的算法，因为他在实现时进行了很多优化处理。</p>
<ul>
<li>xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率</li>
<li>在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。</li>
<li>xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率</li>
</ul>
<p>参考资料：<br>1.李航《统计学习方法》<br>2.周志华《机器学习》<br>3.<a href="http://www.cnblogs.com/wxquare/p/5541414.html" target="_blank" rel="external">一步一步理解GB、GBDT、xgboost</a></p>

      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">Published Date:</span>
          <a href="/2017/07/06/机器学习算法之集成学习boosting/" class="article-date">
  <time datetime="2017-07-06T04:06:41.000Z" itemprop="datePublished">2017-07-06</time>
</a>

        </li>
        
        
          <li>
            <span class="label">Tag:</span>
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">机器学习</a></li></ul>


          </li>
        
        <hr/>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <a href="/2017/07/21/Tensorflow学习笔记/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Tensorflow学习笔记
        
      </div>
    </a>
  
  
    <a href="/2017/04/26/hello-world/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">每个人择食而亡</div>
    </a>
  
</nav>


  
</article>






      </div>
      
    <footer id="footer" class="post-footer footer">
      
      <hr/>
      <div id="footerContent" class="footer-content">
        <p>ipsum dolor sit amet, <strong>consectetur adipiscing elit.</strong> Fusce eget urna vitae velit <em>eleifend interdum at ac nisi. In nec ligula lacus. Cum sociis natoque</em> penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed eu cursus erat, ut dapibus quam. Post</p>


      </div>
    </footer>

      



<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/typing.js"></script>
<!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->







    </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  </body>
</html>
