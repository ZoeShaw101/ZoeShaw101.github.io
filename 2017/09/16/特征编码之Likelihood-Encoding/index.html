<!DOCTYPE html>
<html lang="null">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>特征编码之Likelihood Encoding | Shaw&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="特征编码特征编码为特征工程中经常性的步骤，将类别变量编码成数值型变量，再扔给回归器、分类器处理。常见的编码方式有one-hot encoding、binary encoding等。之前进行构建矩阵、count_value、再将类别变量map的过程手动编码，现在有的模型已经可以自己处理编码（比如catboost），但是很多时候模型自动编码的结果并不理想。
One-Hot Encoding简言之one">
<meta property="og:type" content="article">
<meta property="og:title" content="特征编码之Likelihood Encoding">
<meta property="og:url" content="http://yoursite.com/2017/09/16/特征编码之Likelihood-Encoding/index.html">
<meta property="og:site_name" content="Shaw's Blog">
<meta property="og:description" content="特征编码特征编码为特征工程中经常性的步骤，将类别变量编码成数值型变量，再扔给回归器、分类器处理。常见的编码方式有one-hot encoding、binary encoding等。之前进行构建矩阵、count_value、再将类别变量map的过程手动编码，现在有的模型已经可以自己处理编码（比如catboost），但是很多时候模型自动编码的结果并不理想。
One-Hot Encoding简言之one">
<meta property="og:image" content="https://cdn-images-1.medium.com/max/600/1*3hdYEX5eixaV4F3wT5OmBg.png">
<meta property="og:updated_time" content="2017-09-16T03:34:18.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="特征编码之Likelihood Encoding">
<meta name="twitter:description" content="特征编码特征编码为特征工程中经常性的步骤，将类别变量编码成数值型变量，再扔给回归器、分类器处理。常见的编码方式有one-hot encoding、binary encoding等。之前进行构建矩阵、count_value、再将类别变量map的过程手动编码，现在有的模型已经可以自己处理编码（比如catboost），但是很多时候模型自动编码的结果并不理想。
One-Hot Encoding简言之one">
<meta name="twitter:image" content="https://cdn-images-1.medium.com/max/600/1*3hdYEX5eixaV4F3wT5OmBg.png">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/typing.css">
  
  <link rel="stylesheet" href="/css/font-awesome.min.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>

  
    
      <body>
    
  
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container" class="container">
        <article id="post-特征编码之Likelihood-Encoding" class="article article-type-post" itemscope itemprop="blogPost">
  <header id="header" class="header">
  <nav id="main-nav" class="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
</header>

  <hr/>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      特征编码之Likelihood Encoding
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        <h1 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h1><p>特征编码为特征工程中经常性的步骤，将类别变量编码成数值型变量，再扔给回归器、分类器处理。常见的编码方式有one-hot encoding、binary encoding等。之前进行构建矩阵、count_value、再将类别变量map的过程手动编码，现在有的模型已经可以自己处理编码（比如<a href="https://catboost.yandex/" target="_blank" rel="external">catboost</a>），但是很多时候模型自动编码的结果并不理想。</p>
<h2 id="One-Hot-Encoding"><a href="#One-Hot-Encoding" class="headerlink" title="One-Hot Encoding"></a>One-Hot Encoding</h2><p>简言之one就是将每个类别变量中的出现的值都列出来，然后在出现的对应位置上标1，其余位置标0。如下图所示：<br><img src="https://cdn-images-1.medium.com/max/600/1*3hdYEX5eixaV4F3wT5OmBg.png" alt=""><br>所以其特点是：</p>
<ul>
<li>编码后多增加的列数就是每个类别型变量的取值个数。</li>
<li>会有很多的0，是有少数的1。<br>所以当矩阵很稀疏的时候应该要考虑稀疏表示，但是很多机器学习模型都不是太支持稀疏存储（如lightgbm）。</li>
</ul>
<h2 id="One-Hot-Encoding-Performs-Poorly-in-Tree-Models"><a href="#One-Hot-Encoding-Performs-Poorly-in-Tree-Models" class="headerlink" title="One-Hot Encoding Performs Poorly in Tree Models"></a>One-Hot Encoding Performs Poorly in Tree Models</h2><p>在实际中做比赛时，会发现当类别变量的取值集合非常大时（成千上万个），那么使用one出来的矩阵是非常稀疏的，在集成树模型上的表现就会非常差。那么原因是什么呢？<br>可能原因是ohe后的特征空间很大，那么在sample一些特征作为当前该树用于分裂的特征时，很可能那些重要的特征没有被选上，或者说被选上的概率远远小于没有进行ohe时的概率。<br>所以对于这种取值集合很大的类别型变量，需要寻找其他编码方式。Likelihood Encoding就是其中一种。</p>
<h1 id="Likelihood-Encoding"><a href="#Likelihood-Encoding" class="headerlink" title="Likelihood Encoding"></a>Likelihood Encoding</h1><p>LE又称为Impacted Encoding。其思想主要是将类别型变量用对应的label变量取值的均值来代替。但是这样直接取均值的话，该类别变量就不存在与label直接的信息交换了，也就该变量的信息没能leak给label，所以会导致一些baised estimation。解决办法是使用 <strong><em>KFold</em></strong> 来交叉取值。而且一般取两层的CV。<br>具体过程如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">首先将数据集划分为20folds，然后使用fold #2-20来预测fold #1的值；</div><div class="line">将fold #2-20又️划分一层，划分为10 folds；</div><div class="line">计算这10 folds的out of folds值：比如使用fold #2-10 的对应label的均值，来作为fold #1的预测值；</div><div class="line">得到内层CV的10个均值，再将这10个取平均，则得到了外层fold #1的值；</div><div class="line">依次将外层20个fold都可以计算出来。</div></pre></td></tr></table></figure>
<h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import pandas as pd</div><div class="line">from sklearn.model_selection import KFold</div><div class="line">import dill as pickle</div><div class="line">import sys</div><div class="line"></div><div class="line">def input_data(train_file):</div><div class="line">    with open(train_file, &apos;rb&apos;) as f1:</div><div class="line">       train_data = pickle.load(f1)</div><div class="line">    cat_feature = []</div><div class="line">    for dtype, feature in zip(train_data.dtypes, train_data.columns):</div><div class="line">        if dtype == object:</div><div class="line">            cat_feature.append(feature)</div><div class="line">    print cat_feature</div><div class="line">    return (train_data, cat_feature)</div><div class="line"></div><div class="line">def clean_noise(data):</div><div class="line">    MinLogError = -0.4</div><div class="line">    MaxLogError = 0.418</div><div class="line">    data = data[(data[&apos;logerror&apos;] &gt; MinLogError) &amp; (data[&apos;logerror&apos;] &lt; MaxLogError)]</div><div class="line">    return data</div><div class="line"></div><div class="line">def likelihood_encoding(data, feature, data_type, target = &apos;logerror&apos;):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    :param data:</div><div class="line">    :param feature:</div><div class="line">    :param terget:</div><div class="line">    :return: likelihood encoded data</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    #data = data[f].values.astype(np.str_, copy=False)</div><div class="line">    np.random.seed(2017)</div><div class="line">    n_folds = 10</div><div class="line">    n_inner_folds = 5</div><div class="line">    likelihood_encoded = pd.Series()</div><div class="line">    ##global mean, could be tuned later</div><div class="line">    oof_default_mean = data[target].mean()</div><div class="line">    kf = KFold(n_splits=n_folds, shuffle=True)</div><div class="line">    oof_mean_cv = pd.DataFrame()</div><div class="line">    split = 0</div><div class="line">    print (&apos;raw data shape &#123;&#125;&apos;.format(data.shape))</div><div class="line"></div><div class="line">    for infold, oof in kf.split(data[feature]):</div><div class="line">        #print (&apos;infold data shape %s , oof data shape %s&apos;</div><div class="line">        #       % (data.iloc[infold].shape, data.iloc[oof].shape))</div><div class="line"></div><div class="line">        print (&apos;==============level 1 encoding..., fold %s ============&apos; % split)</div><div class="line">        inner_kf = KFold(n_splits=n_inner_folds, shuffle=True)</div><div class="line">        inner_oof_default_mean = data.iloc[infold][target].mean()</div><div class="line">        inner_split = 0</div><div class="line">        ## inner out of fold mean, used for outer oof</div><div class="line">        inner_oof_mean_cv = pd.DataFrame()</div><div class="line">        ##</div><div class="line">        likelihood_encoded_cv = pd.Series()</div><div class="line">        for inner_infold, inner_oof in inner_kf.split(data.iloc[infold]):</div><div class="line">            #print (&apos;innner infold data shape %s , inner oof data shape %s&apos;</div><div class="line">            #       % (data.iloc[inner_infold].shape, data.iloc[inner_oof].shape))</div><div class="line">            print (&apos;==============level 2 encoding..., inner fold %s ============&apos; % inner_split)</div><div class="line">            ## inner out of fold mean</div><div class="line">            oof_mean = data.iloc[inner_infold].groupby(by=feature)[target].mean()</div><div class="line">            # assign oof_mean to the in fold</div><div class="line">            likelihood_encoded_cv = likelihood_encoded_cv.append(data.iloc[infold].apply(</div><div class="line">                lambda x : oof_mean[x[feature]]</div><div class="line">                if x[feature] in oof_mean.index</div><div class="line">                else inner_oof_default_mean</div><div class="line">                , axis = 1</div><div class="line">            ))</div><div class="line">            inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how=&apos;outer&apos;)</div><div class="line"></div><div class="line">            inner_oof_mean_cv.fillna(inner_oof_default_mean, inplace=True)</div><div class="line">            inner_split += 1</div><div class="line"></div><div class="line">        #print inner_oof_mean_cv.head()</div><div class="line">        #print inner_oof_mean_cv.index</div><div class="line">        #sys.exit(1)</div><div class="line"></div><div class="line">        oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how=&apos;outer&apos;)</div><div class="line"></div><div class="line">        oof_mean_cv.fillna(value=oof_default_mean, inplace=True)</div><div class="line">        split += 1</div><div class="line">        print (&apos;============final mapping...===========&apos;)</div><div class="line"></div><div class="line">        #print type(data.iloc[1][feature][0]), &apos; &apos;, data.iloc[1][feature][0]</div><div class="line">        # print data.iloc[oof][feature]</div><div class="line">        # print(np.mean(inner_oof_mean_cv.ix[&apos;6037&apos;].values))</div><div class="line">        #sys.exit(1)</div><div class="line"></div><div class="line">        likelihood_encoded = likelihood_encoded.append(data.iloc[oof].apply(</div><div class="line">            lambda x: np.mean(inner_oof_mean_cv.ix[x[feature]].values)</div><div class="line">            if x[feature] in inner_oof_mean_cv.index</div><div class="line">            else oof_default_mean</div><div class="line">            , axis=1</div><div class="line">        ))</div><div class="line">    return (likelihood_encoded, oof_mean_cv.mean(axis = 1), oof_default_mean)</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    i_file = &apos;train.pkl&apos;</div><div class="line">    train_data, cat_feature = input_data(i_file)</div><div class="line">    clean_noise(train_data)</div><div class="line">    likelihood_coding_map = &#123;&#125;</div><div class="line"></div><div class="line">    debug_cat_feature = [&apos;fipsid&apos;, &apos;tractid&apos;]</div><div class="line">    # debug_cat_feature = [&apos;blockid&apos;]</div><div class="line"></div><div class="line">    for f in debug_cat_feature:</div><div class="line">        data_type = False</div><div class="line">        print (&apos;Likelihood coding for &#123;&#125;&apos;.format(f))</div><div class="line">        if type(train_data.loc[0][f]) == float:</div><div class="line">            data_type = True</div><div class="line">        train_data[f], likelihood_coding_mapping, default_coding = likelihood_encoding(train_data, f, data_type)</div><div class="line">        #likelihood_coding_map[f] = (likelihood_coding_mapping, default_coding)</div><div class="line">        # mapping, default_mean = likelihood_coding_map[f]</div><div class="line">        # test_data[f] = test_data.apply(lambda x : mapping[x[f]]</div><div class="line">        #                                if x[f] in mapping</div><div class="line">        #                                else default_mean</div><div class="line">        #                                ,axis = 1)</div><div class="line">    print train_data.head()</div><div class="line">    print &apos;=============================================&apos;</div><div class="line"></div><div class="line">    print train_data[&apos;fipsid&apos;].value_counts()</div><div class="line"></div><div class="line">    with open(&apos;encoded_train.pkl&apos;, &apos;wb&apos;) as f1:</div><div class="line">        pickle.dump(train_data, f1, -1)</div><div class="line">    f1.close()</div></pre></td></tr></table></figure>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931" target="_blank" rel="external">Visiting: Categorical Features and Encoding in Decision Trees</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">Published Date:</span>
          <a href="/2017/09/16/特征编码之Likelihood-Encoding/" class="article-date">
  <time datetime="2017-09-16T02:45:47.000Z" itemprop="datePublished">2017-09-16</time>
</a>

        </li>
        
        
          <li>
            <span class="label">Tag:</span>
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">机器学习</a></li></ul>


          </li>
        
        <hr/>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <a href="/2017/09/21/计算语言学基础－读书笔记（一）词法分析/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          计算语言学基础－读书笔记（一）词法分析
        
      </div>
    </a>
  
  
    <a href="/2017/09/03/使用CNN进行网站文本分类/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">使用CNN进行网站文本分类</div>
    </a>
  
</nav>


  
</article>






      </div>
      
    <footer id="footer" class="post-footer footer">
      
        <ul class="footer-links">
          
            <li><a href="/archives/"><span class="fa fa-Blog"></span></a></li>
          
            <li><a href="https://github.com/ZoeShaw101"><span class="fa fa-GitHub"></span></a></li>
          
            <li><a href="http://weibo.com/2817056541/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo"><span class="fa fa-Weibo"></span></a></li>
          
            <li><a href="/atom.xml"><span class="fa fa-rss"></span></a></li>
          
        </ul>
	    
      <hr/>
      <div id="footerContent" class="footer-content">
        <p>ipsum dolor sit amet, <strong>consectetur adipiscing elit.</strong> Fusce eget urna vitae velit <em>eleifend interdum at ac nisi. In nec ligula lacus. Cum sociis natoque</em> penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed eu cursus erat, ut dapibus quam. Post</p>


      </div>
    </footer>

      



<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/typing.js"></script>
<!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->







    </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  </body>
</html>
